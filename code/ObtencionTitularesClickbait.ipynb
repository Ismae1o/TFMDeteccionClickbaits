{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "En este script procesamos el dataset clickbait_dataset_multitoken para extraer los titulares de las noticias gracias a la urls extraÃ­das en el script ObtencionTweetsClickbait.\n",
        "\n"
      ],
      "metadata": {
        "id": "lWTvTSwayZf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "\n",
        "class MultiTokenTwitterProcessor:\n",
        "    def __init__(self, bearer_tokens):\n",
        "        self.bearer_tokens = bearer_tokens\n",
        "        self.token_count = len(bearer_tokens)\n",
        "        self.headers_list = []\n",
        "\n",
        "        # Crear headers para cada token\n",
        "        for token in bearer_tokens:\n",
        "            self.headers_list.append({\n",
        "                'Authorization': f'Bearer {token}',\n",
        "                'Content-Type': 'application/json'\n",
        "            })\n",
        "\n",
        "        self.processed_count = 0\n",
        "        self.start_time = datetime.now()\n",
        "        self.session_stats = {\n",
        "            'successful': 0,\n",
        "            'errors': 0,\n",
        "            'rate_limited': 0,\n",
        "            'not_found': 0\n",
        "        }\n",
        "\n",
        "        # Lock para thread safety\n",
        "        self.stats_lock = threading.Lock()\n",
        "\n",
        "    def extract_tweet_id(self, url):\n",
        "        \"\"\"\n",
        "        Extrae el ID del tweet de una URL de Twitter\n",
        "        \"\"\"\n",
        "        if pd.isna(url) or url == '':\n",
        "            return None\n",
        "\n",
        "        patterns = [\n",
        "            r'twitter\\.com/[^/]+/status/(\\d+)',\n",
        "            r'x\\.com/[^/]+/status/(\\d+)',\n",
        "            r'mobile\\.twitter\\.com/[^/]+/status/(\\d+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, str(url))\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "        return None\n",
        "\n",
        "    def get_tweet_text_with_token(self, tweet_id, token_index):\n",
        "        \"\"\"\n",
        "        Obtiene el texto de un tweet usando un token especÃ­fico\n",
        "        \"\"\"\n",
        "        url = f\"https://api.twitter.com/2/tweets/{tweet_id}\"\n",
        "        params = {\n",
        "            'tweet.fields': 'text,created_at,author_id,public_metrics'\n",
        "        }\n",
        "\n",
        "        headers = self.headers_list[token_index]\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, params=params, timeout=30)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                if 'data' in data:\n",
        "                    with self.stats_lock:\n",
        "                        self.session_stats['successful'] += 1\n",
        "                    return {\n",
        "                        'text': data['data']['text'],\n",
        "                        'status': 'success',\n",
        "                        'token_used': token_index + 1\n",
        "                    }\n",
        "                else:\n",
        "                    with self.stats_lock:\n",
        "                        self.session_stats['errors'] += 1\n",
        "                    return {\n",
        "                        'text': \"Error: No se encontrÃ³ el tweet\",\n",
        "                        'status': 'no_data',\n",
        "                        'token_used': token_index + 1\n",
        "                    }\n",
        "            elif response.status_code == 429:\n",
        "                with self.stats_lock:\n",
        "                    self.session_stats['rate_limited'] += 1\n",
        "                return {\n",
        "                    'text': \"Error: LÃ­mite de rate excedido\",\n",
        "                    'status': 'rate_limited',\n",
        "                    'token_used': token_index + 1\n",
        "                }\n",
        "            elif response.status_code == 404:\n",
        "                with self.stats_lock:\n",
        "                    self.session_stats['not_found'] += 1\n",
        "                return {\n",
        "                    'text': \"Error: Tweet no encontrado o privado\",\n",
        "                    'status': 'not_found',\n",
        "                    'token_used': token_index + 1\n",
        "                }\n",
        "            else:\n",
        "                with self.stats_lock:\n",
        "                    self.session_stats['errors'] += 1\n",
        "                return {\n",
        "                    'text': f\"Error HTTP: {response.status_code}\",\n",
        "                    'status': 'http_error',\n",
        "                    'token_used': token_index + 1\n",
        "                }\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            with self.stats_lock:\n",
        "                self.session_stats['errors'] += 1\n",
        "            return {\n",
        "                'text': \"Error: Timeout de conexiÃ³n\",\n",
        "                'status': 'timeout',\n",
        "                'token_used': token_index + 1\n",
        "            }\n",
        "        except Exception as e:\n",
        "            with self.stats_lock:\n",
        "                self.session_stats['errors'] += 1\n",
        "            return {\n",
        "                'text': f\"Error de conexiÃ³n: {str(e)}\",\n",
        "                'status': 'connection_error',\n",
        "                'token_used': token_index + 1\n",
        "            }\n",
        "\n",
        "    def process_tweet_batch(self, tweet_data_batch):\n",
        "        \"\"\"\n",
        "        Procesa un lote de tweets usando diferentes tokens simultÃ¡neamente\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        # Usar ThreadPoolExecutor para procesar en paralelo\n",
        "        with ThreadPoolExecutor(max_workers=self.token_count) as executor:\n",
        "            # Crear tareas para cada tweet\n",
        "            future_to_data = {}\n",
        "            for i, (idx, url) in enumerate(tweet_data_batch):\n",
        "                tweet_id = self.extract_tweet_id(url)\n",
        "                if tweet_id:\n",
        "                    token_index = i % self.token_count  # Distribuir tokens\n",
        "                    future = executor.submit(self.get_tweet_text_with_token, tweet_id, token_index)\n",
        "                    future_to_data[future] = (idx, url, tweet_id, token_index)\n",
        "                else:\n",
        "                    results[idx] = {\n",
        "                        'text': \"Error: No se pudo extraer ID del tweet\",\n",
        "                        'status': 'invalid_url',\n",
        "                        'token_used': 'N/A'\n",
        "                    }\n",
        "\n",
        "            # Recoger resultados\n",
        "            for future in as_completed(future_to_data):\n",
        "                idx, url, tweet_id, token_index = future_to_data[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    results[idx] = result\n",
        "                except Exception as e:\n",
        "                    results[idx] = {\n",
        "                        'text': f\"Error en thread: {str(e)}\",\n",
        "                        'status': 'thread_error',\n",
        "                        'token_used': token_index + 1\n",
        "                    }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_progress(self, df, batch_num=None, is_final=False):\n",
        "        \"\"\"\n",
        "        Guarda el progreso del procesamiento\n",
        "        \"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        if is_final:\n",
        "            filename = f'dataset_procesado_FINAL_{timestamp}.csv'\n",
        "        elif batch_num:\n",
        "            filename = f'dataset_procesado_lote_{batch_num}_{timestamp}.csv'\n",
        "        else:\n",
        "            filename = f'dataset_procesado_progreso_{timestamp}.csv'\n",
        "\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"ğŸ’¾ Progreso guardado en: {filename}\")\n",
        "        return filename\n",
        "\n",
        "    def load_existing_progress(self, original_df):\n",
        "        \"\"\"\n",
        "        Carga progreso existente si hay archivos de guardado previos\n",
        "        \"\"\"\n",
        "        import glob\n",
        "        import os\n",
        "\n",
        "        # Buscar archivos de progreso existentes\n",
        "        progress_files = glob.glob('dataset_procesado_*.csv')\n",
        "\n",
        "        if not progress_files:\n",
        "            print(\"ğŸ“‚ No se encontraron archivos de progreso previos\")\n",
        "            return original_df\n",
        "\n",
        "        # Encontrar el archivo mÃ¡s reciente\n",
        "        latest_file = max(progress_files, key=os.path.getctime)\n",
        "\n",
        "        try:\n",
        "            print(f\"ğŸ“‚ Cargando progreso desde: {latest_file}\")\n",
        "            df_progress = pd.read_csv(latest_file)\n",
        "\n",
        "            # Verificar que tiene las columnas necesarias\n",
        "            required_cols = ['texto_titular', 'token_usado', 'status_procesamiento']\n",
        "            if all(col in df_progress.columns for col in required_cols):\n",
        "                # Contar tweets procesados\n",
        "                processed_count = df_progress[\n",
        "                    (df_progress['texto_titular'].notna()) &\n",
        "                    (df_progress['texto_titular'] != '') &\n",
        "                    (df_progress['texto_titular'] != 'Error: No se pudo extraer ID del tweet')\n",
        "                ].shape[0]\n",
        "\n",
        "                print(f\"âœ… Progress cargado exitosamente!\")\n",
        "                print(f\"   Tweets ya procesados: {processed_count}\")\n",
        "                return df_progress\n",
        "            else:\n",
        "                print(\"âš ï¸  Archivo de progreso no tiene las columnas necesarias, iniciando desde cero\")\n",
        "                return original_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error al cargar progreso: {str(e)}\")\n",
        "            print(\"   Iniciando desde cero...\")\n",
        "            return original_df\n",
        "\n",
        "    def process_dataset_multi_token(self, df, delay_minutes=15, batch_size=None, max_batches=None, save_every=5):\n",
        "        \"\"\"\n",
        "        Procesa el dataset usando mÃºltiples tokens con guardado automÃ¡tico\n",
        "        \"\"\"\n",
        "        if batch_size is None:\n",
        "            batch_size = self.token_count  # Por defecto, usar tantos tweets como tokens\n",
        "\n",
        "        # Intentar cargar progreso existente\n",
        "        df = self.load_existing_progress(df)\n",
        "\n",
        "        # Crear las columnas si no existen\n",
        "        if 'texto_titular' not in df.columns:\n",
        "            df['texto_titular'] = ''\n",
        "        if 'token_usado' not in df.columns:\n",
        "            df['token_usado'] = ''\n",
        "        if 'status_procesamiento' not in df.columns:\n",
        "            df['status_procesamiento'] = ''\n",
        "\n",
        "        # Encontrar tweets no procesados\n",
        "        mask = (df['texto_titular'].isna()) | (df['texto_titular'] == '') | (df['texto_titular'] == 'Error: No se pudo extraer ID del tweet')\n",
        "        pending_indices = df[mask].index.tolist()\n",
        "\n",
        "        if not pending_indices:\n",
        "            print(\"ğŸ‰ Â¡Todos los tweets ya han sido procesados!\")\n",
        "            final_file = self.save_progress(df, is_final=True)\n",
        "            return df\n",
        "\n",
        "        # Contar tweets ya procesados\n",
        "        total_tweets = len(df)\n",
        "        processed_tweets = total_tweets - len(pending_indices)\n",
        "\n",
        "        print(f\"ğŸ“Š ESTADÃSTICAS INICIALES:\")\n",
        "        print(f\"   Total de URLs: {total_tweets}\")\n",
        "        print(f\"   URLs ya procesadas: {processed_tweets}\")\n",
        "        print(f\"   URLs pendientes: {len(pending_indices)}\")\n",
        "        print(f\"   Progreso actual: {(processed_tweets/total_tweets*100):.1f}%\")\n",
        "        print(f\"   Tokens disponibles: {self.token_count}\")\n",
        "        print(f\"   Tweets por lote: {batch_size}\")\n",
        "        print(f\"   Delay entre lotes: {delay_minutes} minutos\")\n",
        "        print(f\"   Guardado automÃ¡tico cada: {save_every} lotes\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Crear lotes\n",
        "        batches = []\n",
        "        for i in range(0, len(pending_indices), batch_size):\n",
        "            batch_indices = pending_indices[i:i+batch_size]\n",
        "            batch_data = [(idx, df.loc[idx, 'titular_noticia']) for idx in batch_indices]\n",
        "            batches.append(batch_data)\n",
        "\n",
        "        if max_batches:\n",
        "            batches = batches[:max_batches]\n",
        "\n",
        "        print(f\"ğŸ“¦ Se procesarÃ¡n {len(batches)} lotes\")\n",
        "        if max_batches:\n",
        "            print(f\"   (Limitado a {max_batches} lotes por configuraciÃ³n)\")\n",
        "\n",
        "        total_estimated_time = (len(batches) - 1) * delay_minutes\n",
        "        estimated_finish = datetime.now() + timedelta(minutes=total_estimated_time)\n",
        "        print(f\"â±ï¸  Tiempo estimado: {total_estimated_time} minutos\")\n",
        "        print(f\"   FinalizaciÃ³n estimada: {estimated_finish.strftime('%H:%M:%S')}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Procesar lotes\n",
        "        for batch_num, batch_data in enumerate(batches, 1):\n",
        "            print(f\"\\nğŸš€ LOTE {batch_num}/{len(batches)} - {datetime.now().strftime('%H:%M:%S')}\")\n",
        "            print(f\"   Procesando {len(batch_data)} tweets...\")\n",
        "\n",
        "            # Mostrar URLs del lote\n",
        "            for i, (idx, url) in enumerate(batch_data):\n",
        "                print(f\"   [{i+1}] Fila {idx}: {url}\")\n",
        "\n",
        "            batch_start = time.time()\n",
        "            results = self.process_tweet_batch(batch_data)\n",
        "            batch_duration = time.time() - batch_start\n",
        "\n",
        "            # Actualizar DataFrame con resultados\n",
        "            for idx, result in results.items():\n",
        "                df.loc[idx, 'texto_titular'] = result['text']\n",
        "                df.loc[idx, 'token_usado'] = str(result['token_used'])\n",
        "                df.loc[idx, 'status_procesamiento'] = result['status']\n",
        "\n",
        "            # Mostrar resultados del lote\n",
        "            print(f\"\\nğŸ“Š RESULTADOS DEL LOTE {batch_num}:\")\n",
        "            success_count = sum(1 for r in results.values() if r['status'] == 'success')\n",
        "            print(f\"   âœ… Exitosos: {success_count}/{len(batch_data)}\")\n",
        "            print(f\"   â±ï¸  Tiempo del lote: {batch_duration:.1f} segundos\")\n",
        "\n",
        "            # Mostrar algunos ejemplos de texto obtenido\n",
        "            successful_results = [(idx, r) for idx, r in results.items() if r['status'] == 'success']\n",
        "            if successful_results:\n",
        "                print(f\"   ğŸ“ Ejemplos de texto obtenido:\")\n",
        "                for idx, result in successful_results[:2]:  # Mostrar mÃ¡ximo 2 ejemplos\n",
        "                    text_preview = result['text'][:80] + \"...\" if len(result['text']) > 80 else result['text']\n",
        "                    print(f\"      Fila {idx}: {text_preview}\")\n",
        "\n",
        "            # Guardar progreso cada X lotes\n",
        "            if batch_num % save_every == 0:\n",
        "                print(f\"\\nğŸ’¾ Guardando progreso automÃ¡tico (lote {batch_num})...\")\n",
        "                progress_file = self.save_progress(df, batch_num)\n",
        "\n",
        "                # Mostrar estadÃ­sticas de progreso\n",
        "                total_processed_now = len(df) - len(df[(df['texto_titular'].isna()) |\n",
        "                                                      (df['texto_titular'] == '') |\n",
        "                                                      (df['texto_titular'] == 'Error: No se pudo extraer ID del tweet')])\n",
        "                progress_pct = (total_processed_now / len(df)) * 100\n",
        "                print(f\"   ğŸ“ˆ Progreso total: {total_processed_now}/{len(df)} ({progress_pct:.1f}%)\")\n",
        "                print(f\"   ğŸ“ Archivo: {progress_file}\")\n",
        "\n",
        "            # Esperar antes del siguiente lote (excepto en el Ãºltimo)\n",
        "            if batch_num < len(batches):\n",
        "                print(f\"\\nâ¸ï¸  Esperando {delay_minutes} minutos antes del siguiente lote...\")\n",
        "                print(f\"   Siguiente lote a las: {(datetime.now() + timedelta(minutes=delay_minutes)).strftime('%H:%M:%S')}\")\n",
        "\n",
        "                # Guardado de seguridad cada 30 minutos (2 lotes)\n",
        "                if batch_num % 2 == 0:\n",
        "                    backup_file = self.save_progress(df, batch_num, is_final=False)\n",
        "                    print(f\"   ğŸ›¡ï¸  Backup de seguridad: {backup_file}\")\n",
        "\n",
        "                time.sleep(delay_minutes * 60)\n",
        "\n",
        "        # EstadÃ­sticas finales\n",
        "        elapsed_time = datetime.now() - self.start_time\n",
        "        total_processed = sum(self.session_stats.values())\n",
        "\n",
        "        print(f\"\\n\" + \"=\" * 60)\n",
        "        print(f\"ğŸ‰ PROCESAMIENTO COMPLETADO!\")\n",
        "        print(f\"â±ï¸  Tiempo total: {elapsed_time}\")\n",
        "        print(f\"ğŸ“Š ESTADÃSTICAS FINALES:\")\n",
        "        print(f\"   âœ… Exitosos: {self.session_stats['successful']}\")\n",
        "        print(f\"   âŒ Errores: {self.session_stats['errors']}\")\n",
        "        print(f\"   ğŸš« Rate limited: {self.session_stats['rate_limited']}\")\n",
        "        print(f\"   ğŸ” No encontrados: {self.session_stats['not_found']}\")\n",
        "        print(f\"   ğŸ“Š Total procesados en esta sesiÃ³n: {total_processed}\")\n",
        "        print(f\"   ğŸ¯ Tasa de Ã©xito: {(self.session_stats['successful']/total_processed*100):.1f}%\" if total_processed > 0 else \"   ğŸ¯ Tasa de Ã©xito: 0%\")\n",
        "\n",
        "        # Guardar archivo final\n",
        "        final_file = self.save_progress(df, is_final=True)\n",
        "        print(f\"ğŸ’¾ Archivo final guardado: {final_file}\")\n",
        "\n",
        "        # Crear resumen de estados\n",
        "        try:\n",
        "            summary_df = df['status_procesamiento'].value_counts().to_frame()\n",
        "            summary_filename = f'resumen_procesamiento_{datetime.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
        "            summary_df.to_csv(summary_filename)\n",
        "            print(f\"ğŸ“Š Resumen de estados: {summary_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸  No se pudo generar resumen: {str(e)}\")\n",
        "\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return df\n",
        "\n",
        "# CONFIGURACIÃ“N PRINCIPAL\n",
        "BEARER_TOKENS = [\n",
        "    \"AAAAAAAAAAAAAAAAAAAAAO6%2BzQEAAAAAfryhIj%2BIG9KIbx3tMDhj6B0Tbm8%3DKVoE5N6ibJE5Zh8VuuCtGLkTnFe49EQRgjK2QwkuXmIAm7X4MB\",\n",
        "    \"AAAAAAAAAAAAAAAAAAAAAFd62AEAAAAAYOBUuWTu18IxkOTfrOH50r%2BCqfc%3DbE7hjNoebVw9AvKpYDSr2AA4ooNaQO9Vua51nTttslJjndLCQv\",\n",
        "    \"AAAAAAAAAAAAAAAAAAAAALN62AEAAAAArziUSqloB0%2BPSmM5tUuyp4HU%2BGU%3DL0m0vYE2iBgqABL5wUH1KcBlJbBOnhmWEPSYlydgY8eFJOBrCr\",\n",
        "    \"AAAAAAAAAAAAAAAAAAAAAOF62AEAAAAA5aF%2F%2BM%2FgwXyCEDjYjtiCLV8ODqA%3DhicC7ZhR5SDJ6fl4vs6ztRmlMhWLDmftmv2vzkpE1HkUMKu1R2\",\n",
        "    \"AAAAAAAAAAAAAAAAAAAAAOZ62AEAAAAAddltHTioEAGrHkvZlMuNwvuv4tg%3DHl6OodDftl5lFhC9vlykmg4f1H93TkeQ7L5yuN6evXmb94EPXP\"\n",
        "]\n",
        "\n",
        "print(\"ğŸ”§ Inicializando procesador multi-token...\")\n",
        "print(f\"   Tokens configurados: {len(BEARER_TOKENS)}\")\n",
        "\n",
        "# Cargar el dataset\n",
        "print(\"ğŸ“‚ Cargando dataset...\")\n",
        "df = pd.read_csv('clickbait_dataset_multitoken_20250530_155914.csv')\n",
        "\n",
        "print(f\"ğŸ“Š Dataset cargado: {len(df)} filas\")\n",
        "if 'titular_noticia' in df.columns:\n",
        "    valid_urls = df['titular_noticia'].notna().sum()\n",
        "    print(f\"   URLs vÃ¡lidas: {valid_urls}\")\n",
        "else:\n",
        "    print(\"âŒ Error: Columna 'titular_noticia' no encontrada\")\n",
        "    exit()\n",
        "\n",
        "# Crear el procesador\n",
        "processor = MultiTokenTwitterProcessor(BEARER_TOKENS)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸš€ PROCESAMIENTO COMPLETO - MÃšLTIPLES TOKENS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calcular cuÃ¡ntos lotes necesitamos para 454 URLs\n",
        "total_urls = 454\n",
        "batch_size = 5  # 5 tweets por lote (usando los 5 tokens)\n",
        "estimated_batches = (total_urls + batch_size - 1) // batch_size  # Redondeo hacia arriba\n",
        "\n",
        "print(f\"ğŸ“ˆ Para procesar {total_urls} URLs:\")\n",
        "print(f\"   Lotes estimados: {estimated_batches}\")\n",
        "print(f\"   Tiempo estimado: {(estimated_batches-1) * 15} minutos ({((estimated_batches-1) * 15) / 60:.1f} horas)\")\n",
        "\n",
        "# Procesar todo el dataset con guardado automÃ¡tico\n",
        "df_final = processor.process_dataset_multi_token(\n",
        "    df,\n",
        "    delay_minutes=15,    # 15 minutos entre lotes\n",
        "    batch_size=5,        # 5 tweets por lote\n",
        "    max_batches=None,    # Sin lÃ­mite, procesar todo\n",
        "    save_every=5         # Guardar cada 5 lotes (cada 75 minutos)\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ‰ Â¡PROCESAMIENTO TERMINADO!\")\n",
        "print(f\"ğŸ“ Revisa los archivos generados con 'dataset_procesado_' en el nombre\")\n",
        "print(f\"ğŸ”„ Si el proceso se interrumpiÃ³, solo ejecuta el cÃ³digo de nuevo\")\n",
        "print(f\"   y automÃ¡ticamente continuarÃ¡ desde donde se quedÃ³\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RjjA67-oxrX",
        "outputId": "ed896615-21d0-41b1-da01-0dd1afa1b7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Inicializando procesador multi-token...\n",
            "   Tokens configurados: 5\n",
            "ğŸ“‚ Cargando dataset...\n",
            "ğŸ“Š Dataset cargado: 454 filas\n",
            "   URLs vÃ¡lidas: 454\n",
            "\n",
            "============================================================\n",
            "ğŸš€ PROCESAMIENTO COMPLETO - MÃšLTIPLES TOKENS\n",
            "============================================================\n",
            "ğŸ“ˆ Para procesar 454 URLs:\n",
            "   Lotes estimados: 91\n",
            "   Tiempo estimado: 1350 minutos (22.5 horas)\n",
            "ğŸ“‚ Cargando progreso desde: dataset_procesado_lote_34_20250604_210402.csv\n",
            "âœ… Progress cargado exitosamente!\n",
            "   Tweets ya procesados: 400\n",
            "ğŸ“Š ESTADÃSTICAS INICIALES:\n",
            "   Total de URLs: 454\n",
            "   URLs ya procesadas: 400\n",
            "   URLs pendientes: 54\n",
            "   Progreso actual: 88.1%\n",
            "   Tokens disponibles: 5\n",
            "   Tweets por lote: 5\n",
            "   Delay entre lotes: 15 minutos\n",
            "   Guardado automÃ¡tico cada: 5 lotes\n",
            "------------------------------------------------------------\n",
            "ğŸ“¦ Se procesarÃ¡n 11 lotes\n",
            "â±ï¸  Tiempo estimado: 150 minutos\n",
            "   FinalizaciÃ³n estimada: 23:43:30\n",
            "============================================================\n",
            "\n",
            "ğŸš€ LOTE 1/11 - 21:13:30\n",
            "   Procesando 5 tweets...\n",
            "   [1] Fila 400: https://twitter.com/libertaddigital/status/1911428141844677071\n",
            "   [2] Fila 401: https://twitter.com/AristeguiOnline/status/1911172524387324023\n",
            "   [3] Fila 402: https://twitter.com/NoticiasCaracol/status/1911075586077864088\n",
            "   [4] Fila 403: https://twitter.com/adnradiochile/status/1911069294206517314\n",
            "   [5] Fila 404: https://twitter.com/mundodeportivo/status/1910954789476696097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-a60943cbf9a6>:304: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  df.loc[idx, 'token_usado'] = str(result['token_used'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š RESULTADOS DEL LOTE 1:\n",
            "   âœ… Exitosos: 4/5\n",
            "   â±ï¸  Tiempo del lote: 0.2 segundos\n",
            "   ğŸ“ Ejemplos de texto obtenido:\n",
            "      Fila 401: #Videoâ–¶ï¸ | La Ciudad de MÃ©xico confirma quiÃ©n darÃ¡ concierto gratuito en el ZÃ³ca...\n",
            "      Fila 403: No serÃ¡ ni Miranda! ni Rod Stewart: esta es la superestrella mundial que inaugur...\n",
            "\n",
            "â¸ï¸  Esperando 15 minutos antes del siguiente lote...\n",
            "   Siguiente lote a las: 21:28:31\n",
            "\n",
            "ğŸš€ LOTE 2/11 - 21:28:31\n",
            "   Procesando 5 tweets...\n",
            "   [1] Fila 405: https://twitter.com/sextaNoticias/status/1910935919986700779\n",
            "   [2] Fila 406: https://twitter.com/okdiario/status/1910936995896971553\n",
            "   [3] Fila 407: https://twitter.com/ahorrandoclick1/status/1910704116843569578/photo/1\n",
            "   [4] Fila 408: https://twitter.com/soy_502/status/1910703539489268162\n",
            "   [5] Fila 409: https://twitter.com/larazon_es/status/1910580321629675731\n",
            "\n",
            "ğŸ“Š RESULTADOS DEL LOTE 2:\n",
            "   âœ… Exitosos: 2/5\n",
            "   â±ï¸  Tiempo del lote: 0.1 segundos\n",
            "   ğŸ“ Ejemplos de texto obtenido:\n",
            "      Fila 409: ğŸš” La Guardia Civil comenzarÃ¡ a revisar las guanteras de miles de coches.\n",
            "\n",
            "Este e...\n",
            "      Fila 407: Eric Dane https://t.co/mDMH5ORFWm https://t.co/3n7AgXX3Li\n",
            "\n",
            "â¸ï¸  Esperando 15 minutos antes del siguiente lote...\n",
            "   Siguiente lote a las: 21:43:31\n",
            "ğŸ’¾ Progreso guardado en: dataset_procesado_lote_2_20250604_212831.csv\n",
            "   ğŸ›¡ï¸  Backup de seguridad: dataset_procesado_lote_2_20250604_212831.csv\n",
            "\n",
            "ğŸš€ LOTE 3/11 - 21:43:31\n",
            "   Procesando 5 tweets...\n",
            "   [1] Fila 410: https://twitter.com/BBCWorld/status/1910369189912584615\n",
            "   [2] Fila 411: https://twitter.com/VSportsTM/status/1910486705171263638\n",
            "   [3] Fila 412: https://twitter.com/WinSportsTV/status/1910473638219809070\n",
            "   [4] Fila 413: https://twitter.com/3djuegos/status/1910504216029757831\n",
            "   [5] Fila 414: https://twitter.com/okdiario/status/1910219473426010219\n",
            "\n",
            "ğŸ“Š RESULTADOS DEL LOTE 3:\n",
            "   âœ… Exitosos: 1/5\n",
            "   â±ï¸  Tiempo del lote: 0.2 segundos\n",
            "   ğŸ“ Ejemplos de texto obtenido:\n",
            "      Fila 414: AdiÃ³s a los abonos gratis de RENFE si no haces esto: quedan dÃ­as https://t.co/W4...\n",
            "\n",
            "â¸ï¸  Esperando 15 minutos antes del siguiente lote...\n",
            "   Siguiente lote a las: 21:58:31\n",
            "\n",
            "ğŸš€ LOTE 4/11 - 21:58:31\n",
            "   Procesando 5 tweets...\n",
            "   [1] Fila 415: https://twitter.com/Lecturas/status/1910218696573804811\n",
            "   [2] Fila 416: https://twitter.com/clarincom/status/1910140675087008083\n",
            "   [3] Fila 417: https://twitter.com/todonoticias/status/1910070447179288668\n",
            "   [4] Fila 418: https://twitter.com/TheObjective_es/status/1910067694369468645\n",
            "   [5] Fila 419: https://twitter.com/biobio/status/1909974647707217961\n",
            "\n",
            "ğŸ“Š RESULTADOS DEL LOTE 4:\n",
            "   âœ… Exitosos: 1/5\n",
            "   â±ï¸  Tiempo del lote: 0.1 segundos\n",
            "   ğŸ“ Ejemplos de texto obtenido:\n",
            "      Fila 419: Rescatan cÃ¡mara sumergida hace 55 aÃ±os en el Lago Ness para buscar al \"monstruo\"...\n",
            "\n",
            "â¸ï¸  Esperando 15 minutos antes del siguiente lote...\n",
            "   Siguiente lote a las: 22:13:31\n",
            "ğŸ’¾ Progreso guardado en: dataset_procesado_lote_4_20250604_215831.csv\n",
            "   ğŸ›¡ï¸  Backup de seguridad: dataset_procesado_lote_4_20250604_215831.csv\n",
            "\n",
            "ğŸš€ LOTE 5/11 - 22:13:31\n",
            "   Procesando 5 tweets...\n",
            "   [1] Fila 420: https://twitter.com/3djuegos/status/1909963309593350176\n",
            "   [2] Fila 421: https://twitter.com/okdiario/status/1909944389226426806\n",
            "   [3] Fila 422: https://twitter.com/larazon_es/status/1909852556714451392\n",
            "   [4] Fila 423: https://twitter.com/WRadioColombia/status/1909826103050747997\n",
            "   [5] Fila 424: https://twitter.com/adnradiochile/status/1909586020695212174\n",
            "\n",
            "ğŸ“Š RESULTADOS DEL LOTE 5:\n",
            "   âœ… Exitosos: 1/5\n",
            "   â±ï¸  Tiempo del lote: 0.1 segundos\n",
            "   ğŸ“ Ejemplos de texto obtenido:\n",
            "      Fila 424: Nueva reducciÃ³n de la jornada laboral en Chile: desde este dÃ­a se trabajarÃ¡ solo...\n",
            "\n",
            "ğŸ’¾ Guardando progreso automÃ¡tico (lote 5)...\n",
            "ğŸ’¾ Progreso guardado en: dataset_procesado_lote_5_20250604_221331.csv\n",
            "   ğŸ“ˆ Progreso total: 425/454 (93.6%)\n",
            "   ğŸ“ Archivo: dataset_procesado_lote_5_20250604_221331.csv\n",
            "\n",
            "â¸ï¸  Esperando 15 minutos antes del siguiente lote...\n",
            "   Siguiente lote a las: 22:28:31\n",
            "\n",
            "ğŸš€ LOTE 6/11 - 22:28:31\n",
            "   Procesando 5 tweets...\n",
            "   [1] Fila 425: https://twitter.com/AztecaDeportes/status/1909690458806403470\n",
            "   [2] Fila 426: https://twitter.com/genbeta/status/1909613777613365699\n",
            "   [3] Fila 427: https://twitter.com/CaracolRadio/status/1909575055572337020\n",
            "   [4] Fila 428: https://twitter.com/larazon_es/status/1909544245540856203\n",
            "   [5] Fila 429: https://twitter.com/mundodeportivo/status/1909539217639612646\n",
            "\n",
            "ğŸ“Š RESULTADOS DEL LOTE 6:\n",
            "   âœ… Exitosos: 1/5\n",
            "   â±ï¸  Tiempo del lote: 0.2 segundos\n",
            "   ğŸ“ Ejemplos de texto obtenido:\n",
            "      Fila 429: ğŸ”µ Se marchÃ³ del Manchester City y le ha dicho a Guardiola que no quiere volver: ...\n",
            "\n",
            "â¸ï¸  Esperando 15 minutos antes del siguiente lote...\n",
            "   Siguiente lote a las: 22:43:31\n",
            "ğŸ’¾ Progreso guardado en: dataset_procesado_lote_6_20250604_222831.csv\n",
            "   ğŸ›¡ï¸  Backup de seguridad: dataset_procesado_lote_6_20250604_222831.csv\n",
            "\n",
            "ğŸš€ LOTE 7/11 - 22:43:31\n",
            "   Procesando 5 tweets...\n",
            "   [1] Fila 430: https://twitter.com/ahorrandoclick1/status/1909539704707444924/photo/1\n",
            "   [2] Fila 431: https://twitter.com/Lecturas/status/1909539220781219915\n",
            "   [3] Fila 432: https://twitter.com/andro4all/status/1909539311684313546\n",
            "   [4] Fila 433: https://twitter.com/infobae/status/1909534794783863200\n",
            "   [5] Fila 434: https://twitter.com/ahorrandoclick1/status/1909535608298480095/photo/1\n",
            "\n",
            "ğŸ“Š RESULTADOS DEL LOTE 7:\n",
            "   âœ… Exitosos: 1/5\n",
            "   â±ï¸  Tiempo del lote: 0.1 segundos\n",
            "   ğŸ“ Ejemplos de texto obtenido:\n",
            "      Fila 434: El Ebro. https://t.co/nbjURYDQEc\n",
            "\n",
            "â¸ï¸  Esperando 15 minutos antes del siguiente lote...\n",
            "   Siguiente lote a las: 22:58:31\n",
            "\n",
            "ğŸš€ LOTE 8/11 - 22:58:31\n",
            "   Procesando 5 tweets...\n",
            "   [1] Fila 435: https://twitter.com/ahorrandoclick1/status/1909535036262559803/photo/1\n",
            "   [2] Fila 436: https://twitter.com/quincemil15000/status/1909218989055566287\n",
            "   [3] Fila 437: https://twitter.com/okdiario/status/1909476853728854138\n",
            "   [4] Fila 438: https://twitter.com/larazon_es/status/1909388956589847018\n",
            "   [5] Fila 439: https://twitter.com/infobae/status/1908788263974023203\n",
            "\n",
            "ğŸ“Š RESULTADOS DEL LOTE 8:\n",
            "   âœ… Exitosos: 1/5\n",
            "   â±ï¸  Tiempo del lote: 0.2 segundos\n",
            "   ğŸ“ Ejemplos de texto obtenido:\n",
            "      Fila 439: CÃ³mo activar el modo Minecraft en WhatsApp para disfrutar aÃºn mÃ¡s la pelÃ­cula | ...\n",
            "\n",
            "â¸ï¸  Esperando 15 minutos antes del siguiente lote...\n",
            "   Siguiente lote a las: 23:13:32\n",
            "ğŸ’¾ Progreso guardado en: dataset_procesado_lote_8_20250604_225832.csv\n",
            "   ğŸ›¡ï¸  Backup de seguridad: dataset_procesado_lote_8_20250604_225832.csv\n",
            "\n",
            "ğŸš€ LOTE 9/11 - 23:13:32\n",
            "   Procesando 5 tweets...\n",
            "   [1] Fila 440: https://twitter.com/eldebate_com/status/1909194440293429636\n",
            "   [2] Fila 441: https://twitter.com/abc_es/status/1908923907756998854\n",
            "   [3] Fila 442: https://twitter.com/VandalOnline/status/1909142979773596045\n",
            "   [4] Fila 443: https://twitter.com/AlfaBetaJuega/status/1909143030466064713\n",
            "   [5] Fila 444: https://twitter.com/larepublica_pe/status/1909029358787797253\n",
            "\n",
            "ğŸ“Š RESULTADOS DEL LOTE 9:\n",
            "   âœ… Exitosos: 1/5\n",
            "   â±ï¸  Tiempo del lote: 0.1 segundos\n",
            "   ğŸ“ Ejemplos de texto obtenido:\n",
            "      Fila 444: âš ï¸ ğŸ”´ Senamhi lanza alerta roja desde el 7 de abril por fenÃ³meno de gran magnitud...\n",
            "\n",
            "â¸ï¸  Esperando 15 minutos antes del siguiente lote...\n",
            "   Siguiente lote a las: 23:28:32\n",
            "\n",
            "ğŸš€ LOTE 10/11 - 23:28:32\n",
            "   Procesando 5 tweets...\n",
            "   [1] Fila 445: https://twitter.com/Portafolioco/status/1909109138417655846\n",
            "   [2] Fila 446: https://twitter.com/tndeportivo/status/1908880792291312086\n",
            "   [3] Fila 447: https://twitter.com/tndeportivo/status/1908880725794750482\n",
            "   [4] Fila 448: https://twitter.com/CanalRCN/status/1908525115203105080\n",
            "   [5] Fila 449: https://twitter.com/elgraficoweb/status/1908655894117646794\n",
            "\n",
            "ğŸ“Š RESULTADOS DEL LOTE 10:\n",
            "   âœ… Exitosos: 1/5\n",
            "   â±ï¸  Tiempo del lote: 0.1 segundos\n",
            "   ğŸ“ Ejemplos de texto obtenido:\n",
            "      Fila 449: Bayern MÃºnich anunciÃ³ la despedida de una leyenda que jugarÃ¡ ante Boca en el Mun...\n",
            "\n",
            "ğŸ’¾ Guardando progreso automÃ¡tico (lote 10)...\n",
            "ğŸ’¾ Progreso guardado en: dataset_procesado_lote_10_20250604_232832.csv\n",
            "   ğŸ“ˆ Progreso total: 450/454 (99.1%)\n",
            "   ğŸ“ Archivo: dataset_procesado_lote_10_20250604_232832.csv\n",
            "\n",
            "â¸ï¸  Esperando 15 minutos antes del siguiente lote...\n",
            "   Siguiente lote a las: 23:43:32\n",
            "ğŸ’¾ Progreso guardado en: dataset_procesado_lote_10_20250604_232832.csv\n",
            "   ğŸ›¡ï¸  Backup de seguridad: dataset_procesado_lote_10_20250604_232832.csv\n",
            "\n",
            "ğŸš€ LOTE 11/11 - 23:43:32\n",
            "   Procesando 4 tweets...\n",
            "   [1] Fila 450: https://twitter.com/La_SER/status/1908447797638017393\n",
            "   [2] Fila 451: https://twitter.com/GizmodoES/status/1908137269678027262\n",
            "   [3] Fila 452: https://twitter.com/ahorrandoclick1/status/1908138501759029670/photo/1\n",
            "   [4] Fila 453: https://twitter.com/elEconomistaes/status/1908135374800441351\n",
            "\n",
            "ğŸ“Š RESULTADOS DEL LOTE 11:\n",
            "   âœ… Exitosos: 0/4\n",
            "   â±ï¸  Tiempo del lote: 0.1 segundos\n",
            "\n",
            "============================================================\n",
            "ğŸ‰ PROCESAMIENTO COMPLETADO!\n",
            "â±ï¸  Tiempo total: 2:30:01.792948\n",
            "ğŸ“Š ESTADÃSTICAS FINALES:\n",
            "   âœ… Exitosos: 14\n",
            "   âŒ Errores: 0\n",
            "   ğŸš« Rate limited: 40\n",
            "   ğŸ” No encontrados: 0\n",
            "   ğŸ“Š Total procesados en esta sesiÃ³n: 54\n",
            "   ğŸ¯ Tasa de Ã©xito: 25.9%\n",
            "ğŸ’¾ Progreso guardado en: dataset_procesado_FINAL_20250604_234332.csv\n",
            "ğŸ’¾ Archivo final guardado: dataset_procesado_FINAL_20250604_234332.csv\n",
            "ğŸ“Š Resumen de estados: resumen_procesamiento_20250604_2343.csv\n",
            "============================================================\n",
            "\n",
            "ğŸ‰ Â¡PROCESAMIENTO TERMINADO!\n",
            "ğŸ“ Revisa los archivos generados con 'dataset_procesado_' en el nombre\n",
            "ğŸ”„ Si el proceso se interrumpiÃ³, solo ejecuta el cÃ³digo de nuevo\n",
            "   y automÃ¡ticamente continuarÃ¡ desde donde se quedÃ³\n"
          ]
        }
      ]
    }
  ]
}